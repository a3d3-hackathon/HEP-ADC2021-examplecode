{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f79dab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "BATCH_SIZE = 8192\n",
    "LATENT_DIM = 32\n",
    "VALID_FRAC = 0.25\n",
    "SEED = 101588\n",
    "NUM_TRAIN_SAMPLES = 5000000\n",
    "\n",
    "GEN_UPDATES = 8\n",
    "D_LR = 0.0003\n",
    "G_LR = 0.0008\n",
    "FPR_THRESH = 1e-5\n",
    "\n",
    "BACKGROUND_FNAME = Path(\"background.h5\")\n",
    "SIGNAL_FNAMES = {\n",
    "    \"A-4_leptons\": \"https://zenodo.org/record/7152590/files/Ato4l_lepFilter_13TeV_filtered.h5?download=1\",\n",
    "    \"leptoquarks-b_tau\": \"https://zenodo.org/record/7152599/files/leptoquark_LOWMASS_lepFilter_13TeV_filtered.h5?download=1\",\n",
    "    \"h_0-tau_tau\": \"https://zenodo.org/record/7152614/files/hToTauTau_13TeV_PU20_filtered.h5?download=1\",\n",
    "    \"h_plus-tau_nu\": \"https://zenodo.org/record/7152617/files/hChToTauNu_13TeV_PU20_filtered.h5?download=1\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14753bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_dataset(fname, url):\n",
    "    subprocess.run(f\"wget -O {fname} {url}\", shell=True)\n",
    "    \n",
    "if not BACKGROUND_FNAME.exists():\n",
    "    download_dataset(\n",
    "        str(BACKGROUND_FNAME),\n",
    "        \"https://zenodo.org/record/5046428/files/background_for_training.h5?download=1\"\n",
    "    )\n",
    "\n",
    "for signal, url in SIGNAL_FNAMES.items():\n",
    "    fname = signal + \".h5\"\n",
    "    if not Path(fname).exists():\n",
    "        download_dataset(fname, url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58a3025a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: Pt, Eta, Phi, Class\n",
      "Event types: MET_class_1, Four_Ele_class_2, Four_Mu_class_3, Ten_Jet_class_4\n",
      "Total background events: 13451915\n",
      "Loading 5000000 events\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(dataset, N):\n",
    "    X = dataset[:N]\n",
    "    X, y = np.split(X, [3], axis=-1)\n",
    "    return X, y[:, :, 0]\n",
    "\n",
    "\n",
    "with h5py.File(BACKGROUND_FNAME, \"r\") as f:\n",
    "    print(\"Features:\", \", \".join([i.decode() for i in f[\"Particles_Names\"][:]]))\n",
    "    print(\"Event types:\", \", \".join([i.decode() for i in f[\"Particles_Classes\"][:]]))\n",
    "    print(\"Total background events:\", len(f[\"Particles\"]))\n",
    "    print(f\"Loading {NUM_TRAIN_SAMPLES} events\")\n",
    "    X, masks = load_dataset(f[\"Particles\"], NUM_TRAIN_SAMPLES)\n",
    "\n",
    "_, num_events, num_features = X.shape\n",
    "FEATURE_DIM = num_events * num_features\n",
    "X = X.reshape(-1, FEATURE_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f97b9fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    train_bg_events,\n",
    "    valid_bg_events,\n",
    "    train_bg_masks,\n",
    "    valid_bg_masks\n",
    ") = train_test_split(X, masks, test_size=VALID_FRAC, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5a6fa374",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(train_bg_events)\n",
    "\n",
    "def preprocess(X, mask):\n",
    "    X = scaler.transform(X)\n",
    "    X = X.reshape(-1, num_events, num_features)\n",
    "    X[mask == 0] *= 0\n",
    "    X = X.reshape(-1, FEATURE_DIM)\n",
    "    return X\n",
    "\n",
    "train_bg_events = preprocess(train_bg_events, train_bg_masks)\n",
    "valid_bg_events = preprocess(valid_bg_events, valid_bg_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e401d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 13992 events from signal A-4_leptons\n",
      "Loading 85136 events from signal leptoquarks-b_tau\n",
      "Loading 172820 events from signal h_0-tau_tau\n",
      "Loading 190068 events from signal h_plus-tau_nu\n"
     ]
    }
   ],
   "source": [
    "valid_signal_events = []\n",
    "valid_signal_masks = []\n",
    "\n",
    "for signal in SIGNAL_FNAMES:\n",
    "    with h5py.File(signal + \".h5\", \"r\") as f:\n",
    "        dataset = f[\"Particles\"]\n",
    "        n = int(0.25 * len(dataset))\n",
    "        print(f\"Loading {n} events from signal {signal}\")\n",
    "        events, masks = load_dataset(dataset, n)\n",
    "    events = events.reshape(-1, FEATURE_DIM)\n",
    "    events = preprocess(events, masks)\n",
    "    valid_signal_events.append(events)\n",
    "    valid_signal_masks.append(masks)\n",
    "\n",
    "valid_signal_events = np.concatenate(valid_signal_events)\n",
    "valid_signal_masks = np.concatenate(valid_signal_masks)\n",
    "\n",
    "valid_y = np.concatenate([\n",
    "    np.zeros((len(valid_bg_events), 1)),\n",
    "    np.ones((len(valid_signal_events), 1))\n",
    "])\n",
    "valid_events = np.concatenate([valid_bg_events, valid_signal_events])\n",
    "valid_masks = np.concatenate([valid_bg_masks, valid_signal_masks])\n",
    "\n",
    "idx = np.random.permutation(len(valid_y))\n",
    "valid_y = valid_y[idx]\n",
    "valid_events = valid_events[idx]\n",
    "valid_masks = valid_masks[idx]\n",
    "valid_X = (valid_events, valid_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ae6da00",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_input = tf.keras.Input((LATENT_DIM,))\n",
    "x = tf.keras.layers.Dense(128, activation=\"relu\")(generator_input)\n",
    "x = tf.keras.layers.Dense(256, activation=\"relu\")(x)\n",
    "generator_output = tf.keras.layers.Dense(FEATURE_DIM, activation=\"linear\")(x)\n",
    "generator_mask = tf.keras.layers.Dense(num_events, activation=\"sigmoid\")(x)\n",
    "generator = tf.keras.Model(\n",
    "    inputs=generator_input,\n",
    "    outputs=[generator_output, generator_mask]\n",
    ")\n",
    "\n",
    "discriminator_input = tf.keras.Input((FEATURE_DIM,))\n",
    "disc_x = tf.keras.layers.Dense(256, activation=\"relu\")(discriminator_input)\n",
    "disc_x = tf.keras.layers.Dense(128, activation=\"relu\")(disc_x)\n",
    "disc_x = tf.keras.layers.Dense(64, activation=\"relu\")(disc_x)\n",
    "\n",
    "discriminator_mask = tf.keras.Input((num_events,))\n",
    "disc_mask = tf.keras.layers.Dense(256, activation=\"relu\")(discriminator_mask)\n",
    "disc_mask = tf.keras.layers.Dense(128, activation=\"relu\")(disc_mask)\n",
    "disc_mask = tf.keras.layers.Dense(64, activation=\"relu\")(disc_mask)\n",
    "\n",
    "disc_x = tf.keras.layers.Concatenate()([disc_x, disc_mask])\n",
    "disc_x = tf.keras.layers.Dense(256, activation=\"relu\")(disc_x)\n",
    "disc_x = tf.keras.layers.Dense(512, activation=\"relu\")(disc_x)\n",
    "disc_x = tf.keras.layers.Dense(1, activation=\"linear\")(disc_x)\n",
    "discriminator = tf.keras.Model(\n",
    "    inputs=[discriminator_input, discriminator_mask],\n",
    "    outputs=disc_x\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7c4bdf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TPR(tf.keras.metrics.Metric):\n",
    "    def __init__(self, k, **kwargs):\n",
    "        self.k = k\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.background_preds = tf.convert_to_tensor(())\n",
    "        self.signal_preds = tf.convert_to_tensor(())\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        background = y_pred[y_true == 0]\n",
    "        self.background_preds = tf.concat([self.background_preds, background], axis=0)\n",
    "\n",
    "        signal = y_pred[y_true == 1]\n",
    "        self.signal_preds = tf.concat([self.signal_preds, signal], axis=0)\n",
    "\n",
    "    def result(self):\n",
    "        k = tf.shape(self.background_preds)[0] - self.k\n",
    "        threshold = tf.sort(self.background_preds)[k]\n",
    "        mask = self.signal_preds > threshold\n",
    "        mask = tf.cast(mask, tf.int64)\n",
    "        tpr = tf.math.reduce_mean(mask)\n",
    "        return tpr\n",
    "\n",
    "threshold_k = int(FPR_THRESH * len(valid_bg_events))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2bb36f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def fudge_mask(mask, noisy=True):\n",
    "    mask = tf.where(mask == 0, -10., 10.)\n",
    "    if noisy:\n",
    "        mask = mask + tf.random.normal(shape=(len(mask), num_events))\n",
    "    return tf.sigmoid(mask)\n",
    "\n",
    "\n",
    "class GAN(tf.keras.Model):\n",
    "    def __init__(self, discriminator, generator, latent_dim, gen_updates):\n",
    "        super().__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "        self.gen_updates = gen_updates\n",
    "\n",
    "        self.d_loss_tracker = tf.keras.metrics.Mean(name=\"d_loss\")\n",
    "        self.g_loss_tracker = tf.keras.metrics.Mean(name=\"g_loss\")\n",
    "        self.tpr = TPR(threshold_k)\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
    "        super().compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "    def step_generator(self, batch_size):\n",
    "        # Sample random points in the latent space\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "\n",
    "        # Assemble labels that say \"all real images\"\n",
    "        misleading_labels = tf.zeros((batch_size, 1))\n",
    "\n",
    "        # Train the generator (note that we should *not* update the weights\n",
    "        # of the discriminator)!\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = discriminator(generator(random_latent_vectors))\n",
    "            g_loss = loss_fn(misleading_labels, predictions)\n",
    "        grads = tape.gradient(g_loss, generator.trainable_weights)\n",
    "        g_optimizer.apply_gradients(zip(grads, generator.trainable_weights))\n",
    "        return g_loss\n",
    "\n",
    "    def train_step(self, X):\n",
    "        real_events, real_mask = X\n",
    "        batch_size = tf.shape(real_events)[0]\n",
    "\n",
    "        # train the generator for multiple steps\n",
    "        # in between a single step of the discriminator\n",
    "        g_loss = 0\n",
    "        for i in range(self.gen_updates):\n",
    "            g_loss += self.step_generator(batch_size)\n",
    "        g_loss /= self.gen_updates\n",
    "\n",
    "        # Sample random points in the latent space\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "\n",
    "        # Decode them to fake events\n",
    "        generated_events, generated_mask = generator(random_latent_vectors)\n",
    "\n",
    "        # Combine them with real events\n",
    "        combined_events = tf.concat([generated_events, real_events], axis=0)\n",
    "        combined_masks = tf.concat([generated_mask, real_mask], axis=0)\n",
    "\n",
    "        # Assemble labels discriminating real from fake events\n",
    "        labels = tf.concat(\n",
    "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
    "        )\n",
    "\n",
    "        # Add random noise to the labels - important trick!\n",
    "        labels += 0.05 * tf.random.uniform((2 * batch_size, 1))\n",
    "\n",
    "        # Train the discriminator\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = discriminator([combined_events, combined_masks])\n",
    "            d_loss = loss_fn(labels, predictions)\n",
    "        grads = tape.gradient(d_loss, discriminator.trainable_weights)\n",
    "        d_optimizer.apply_gradients(zip(grads, discriminator.trainable_weights))\n",
    "\n",
    "        self.d_loss_tracker.update_state(d_loss)\n",
    "        self.g_loss_tracker.update_state(g_loss)\n",
    "        return {\n",
    "            \"d_loss\": self.d_loss_tracker.result(),\n",
    "            \"g_loss\": self.g_loss_tracker.result(),\n",
    "        }\n",
    "\n",
    "    def test_step(self, data):\n",
    "        # Unpack the data\n",
    "        x, y = data\n",
    "        y = fudge_mask(y, noisy=False)\n",
    "\n",
    "        # Compute predictions\n",
    "        y_pred = self.discriminator(x, training=False)\n",
    "\n",
    "        # Updates the metrics tracking the loss\n",
    "        self.tpr.update_state(y, y_pred)\n",
    "        return {\"tpr\": self.tpr.result()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a1b030f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_optimizer = tf.keras.optimizers.Adam(learning_rate=D_LR)\n",
    "g_optimizer = tf.keras.optimizers.Adam(learning_rate=G_LR)\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "\n",
    "gan = GAN(discriminator, generator, LATENT_DIM, GEN_UPDATES)\n",
    "gan.compile(d_optimizer, g_optimizer, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "faaa13bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "458/458 [==============================] - ETA: 0s - d_loss: 1.2056 - g_loss: 0.4745"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 14:10:24.410789: W tensorflow/core/framework/op_kernel.cc:1828] OP_REQUIRES failed at strided_slice_op.cc:117 : INVALID_ARGUMENT: slice index -12 of dimension 0 out of bounds.\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "in user code:\n\n    File \"/local/alec.gunny/ipykernel_2153706/1928866576.py\", line 19, in result  *\n        threshold = tf.sort(self.background_preds)[k]\n\n    InvalidArgumentError: {{function_node __wrapped__StridedSlice_device_/job:localhost/replica:0/task:0/device:CPU:0}} slice index -12 of dimension 0 out of bounds. [Op:StridedSlice] name: strided_slice/\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m dataset \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mzip((events, masks))\n\u001b[1;32m     12\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mshuffle(buffer_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m)\u001b[38;5;241m.\u001b[39mbatch(BATCH_SIZE)\u001b[38;5;241m.\u001b[39mmap(fudge_mask)\n\u001b[0;32m---> 13\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mgan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalid_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_y\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/hackathon-F1z9dWsj-py3.9/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/hackathon-F1z9dWsj-py3.9/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:693\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m    692\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 693\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[1;32m    694\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    695\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: in user code:\n\n    File \"/local/alec.gunny/ipykernel_2153706/1928866576.py\", line 19, in result  *\n        threshold = tf.sort(self.background_preds)[k]\n\n    InvalidArgumentError: {{function_node __wrapped__StridedSlice_device_/job:localhost/replica:0/task:0/device:CPU:0}} slice index -12 of dimension 0 out of bounds. [Op:StridedSlice] name: strided_slice/\n"
     ]
    }
   ],
   "source": [
    "events = tf.data.Dataset.from_tensor_slices(train_bg_events.astype(\"float32\"))\n",
    "masks = tf.data.Dataset.from_tensor_slices(train_bg_masks.astype(\"float32\"))\n",
    "dataset = tf.data.Dataset.zip((events, masks))\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(BATCH_SIZE).map(fudge_mask)\n",
    "history = gan.fit(\n",
    "    dataset,\n",
    "    epochs=100,\n",
    "    validation_data=(valid_X, valid_y),\n",
    "    validation_batch_size=4 * BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8ee8c546",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 14:12:26.198943: W tensorflow/core/framework/op_kernel.cc:1828] OP_REQUIRES failed at strided_slice_op.cc:117 : INVALID_ARGUMENT: slice index -12 of dimension 0 out of bounds.\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'strided_slice_1' defined at (most recent call last):\n    File \"/home/alec.gunny/miniconda3/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/home/alec.gunny/miniconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/home/alec.gunny/miniconda3/envs/hackathon-F1z9dWsj-py3.9/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/alec.gunny/miniconda3/envs/hackathon-F1z9dWsj-py3.9/lib/python3.9/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/home/alec.gunny/miniconda3/envs/hackathon-F1z9dWsj-py3.9/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 728, in start\n      self.io_loop.start()\n    File \"/home/alec.gunny/miniconda3/envs/hackathon-F1z9dWsj-py3.9/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"/home/alec.gunny/miniconda3/lib/python3.9/asyncio/base_events.py\", line 596, in run_forever\n      self._run_once()\n    File \"/home/alec.gunny/miniconda3/lib/python3.9/asyncio/base_events.py\", line 1890, in _run_once\n      handle._run()\n    File \"/home/alec.gunny/miniconda3/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/alec.gunny/miniconda3/envs/hackathon-F1z9dWsj-py3.9/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n      await self.process_one()\n    File \"/home/alec.gunny/miniconda3/envs/hackathon-F1z9dWsj-py3.9/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n      await dispatch(*args)\n    File \"/home/alec.gunny/miniconda3/envs/hackathon-F1z9dWsj-py3.9/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n      await result\n    File \"/home/alec.gunny/miniconda3/envs/hackathon-F1z9dWsj-py3.9/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n      reply_content = await reply_content\n    File \"/home/alec.gunny/miniconda3/envs/hackathon-F1z9dWsj-py3.9/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"/home/alec.gunny/miniconda3/envs/hackathon-F1z9dWsj-py3.9/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/alec.gunny/miniconda3/envs/hackathon-F1z9dWsj-py3.9/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"/home/alec.gunny/miniconda3/envs/hackathon-F1z9dWsj-py3.9/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"/home/alec.gunny/miniconda3/envs/hackathon-F1z9dWsj-py3.9/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/alec.gunny/miniconda3/envs/hackathon-F1z9dWsj-py3.9/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/alec.gunny/miniconda3/envs/hackathon-F1z9dWsj-py3.9/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/alec.gunny/miniconda3/envs/hackathon-F1z9dWsj-py3.9/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/local/alec.gunny/ipykernel_2153706/1120452504.py\", line 1, in <module>\n      gan.evaluate(valid_X, valid_y, batch_size=8192)\n    File \"/home/alec.gunny/miniconda3/envs/hackathon-F1z9dWsj-py3.9/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/alec.gunny/miniconda3/envs/hackathon-F1z9dWsj-py3.9/lib/python3.9/site-packages/keras/src/engine/training.py\", line 2200, in evaluate\n      logs = test_function_runner.run_step(\n    File \"/home/alec.gunny/miniconda3/envs/hackathon-F1z9dWsj-py3.9/lib/python3.9/site-packages/keras/src/engine/training.py\", line 4000, in run_step\n      tmp_logs = self._function(dataset_or_iterator)\n    File \"/home/alec.gunny/miniconda3/envs/hackathon-F1z9dWsj-py3.9/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1972, in test_function\n      return step_function(self, iterator)\n    File \"/home/alec.gunny/miniconda3/envs/hackathon-F1z9dWsj-py3.9/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1956, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/alec.gunny/miniconda3/envs/hackathon-F1z9dWsj-py3.9/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1944, in run_step\n      outputs = model.test_step(data)\n    File \"/local/alec.gunny/ipykernel_2153706/4190966585.py\", line 96, in test_step\n      return {\"tpr\": self.tpr.result()}\n    File \"/home/alec.gunny/miniconda3/envs/hackathon-F1z9dWsj-py3.9/lib/python3.9/site-packages/keras/src/utils/metrics_utils.py\", line 137, in decorated\n      raw_result = result_fn(*args)\n    File \"/home/alec.gunny/miniconda3/envs/hackathon-F1z9dWsj-py3.9/lib/python3.9/site-packages/keras/src/metrics/base_metric.py\", line 159, in result_fn\n      return ag_result(*args, **kwargs)\n    File \"/local/alec.gunny/ipykernel_2153706/1928866576.py\", line 19, in result\n      threshold = tf.sort(self.background_preds)[k]\nNode: 'strided_slice_1'\nslice index -12 of dimension 0 out of bounds.\n\t [[{{node strided_slice_1}}]] [Op:__inference_test_function_27418]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalid_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8192\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/hackathon-F1z9dWsj-py3.9/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/hackathon-F1z9dWsj-py3.9/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'strided_slice_1' defined at (most recent call last):\n    File \"/home/alec.gunny/miniconda3/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/home/alec.gunny/miniconda3/lib/python3.9/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/home/alec.gunny/miniconda3/envs/hackathon-F1z9dWsj-py3.9/lib/python3.9/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/alec.gunny/miniconda3/envs/hackathon-F1z9dWsj-py3.9/lib/python3.9/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/home/alec.gunny/miniconda3/envs/hackathon-F1z9dWsj-py3.9/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 728, in start\n      self.io_loop.start()\n    File \"/home/alec.gunny/miniconda3/envs/hackathon-F1z9dWsj-py3.9/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"/home/alec.gunny/miniconda3/lib/python3.9/asyncio/base_events.py\", line 596, in run_forever\n      self._run_once()\n    File \"/home/alec.gunny/miniconda3/lib/python3.9/asyncio/base_events.py\", line 1890, in _run_once\n      handle._run()\n    File \"/home/alec.gunny/miniconda3/lib/python3.9/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/alec.gunny/miniconda3/envs/hackathon-F1z9dWsj-py3.9/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n      await self.process_one()\n    File \"/home/alec.gunny/miniconda3/envs/hackathon-F1z9dWsj-py3.9/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n      await dispatch(*args)\n    File \"/home/alec.gunny/miniconda3/envs/hackathon-F1z9dWsj-py3.9/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n      await result\n    File \"/home/alec.gunny/miniconda3/envs/hackathon-F1z9dWsj-py3.9/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n      reply_content = await reply_content\n    File \"/home/alec.gunny/miniconda3/envs/hackathon-F1z9dWsj-py3.9/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"/home/alec.gunny/miniconda3/envs/hackathon-F1z9dWsj-py3.9/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/alec.gunny/miniconda3/envs/hackathon-F1z9dWsj-py3.9/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"/home/alec.gunny/miniconda3/envs/hackathon-F1z9dWsj-py3.9/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"/home/alec.gunny/miniconda3/envs/hackathon-F1z9dWsj-py3.9/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/alec.gunny/miniconda3/envs/hackathon-F1z9dWsj-py3.9/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/alec.gunny/miniconda3/envs/hackathon-F1z9dWsj-py3.9/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/alec.gunny/miniconda3/envs/hackathon-F1z9dWsj-py3.9/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/local/alec.gunny/ipykernel_2153706/1120452504.py\", line 1, in <module>\n      gan.evaluate(valid_X, valid_y, batch_size=8192)\n    File \"/home/alec.gunny/miniconda3/envs/hackathon-F1z9dWsj-py3.9/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/alec.gunny/miniconda3/envs/hackathon-F1z9dWsj-py3.9/lib/python3.9/site-packages/keras/src/engine/training.py\", line 2200, in evaluate\n      logs = test_function_runner.run_step(\n    File \"/home/alec.gunny/miniconda3/envs/hackathon-F1z9dWsj-py3.9/lib/python3.9/site-packages/keras/src/engine/training.py\", line 4000, in run_step\n      tmp_logs = self._function(dataset_or_iterator)\n    File \"/home/alec.gunny/miniconda3/envs/hackathon-F1z9dWsj-py3.9/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1972, in test_function\n      return step_function(self, iterator)\n    File \"/home/alec.gunny/miniconda3/envs/hackathon-F1z9dWsj-py3.9/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1956, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/alec.gunny/miniconda3/envs/hackathon-F1z9dWsj-py3.9/lib/python3.9/site-packages/keras/src/engine/training.py\", line 1944, in run_step\n      outputs = model.test_step(data)\n    File \"/local/alec.gunny/ipykernel_2153706/4190966585.py\", line 96, in test_step\n      return {\"tpr\": self.tpr.result()}\n    File \"/home/alec.gunny/miniconda3/envs/hackathon-F1z9dWsj-py3.9/lib/python3.9/site-packages/keras/src/utils/metrics_utils.py\", line 137, in decorated\n      raw_result = result_fn(*args)\n    File \"/home/alec.gunny/miniconda3/envs/hackathon-F1z9dWsj-py3.9/lib/python3.9/site-packages/keras/src/metrics/base_metric.py\", line 159, in result_fn\n      return ag_result(*args, **kwargs)\n    File \"/local/alec.gunny/ipykernel_2153706/1928866576.py\", line 19, in result\n      threshold = tf.sort(self.background_preds)[k]\nNode: 'strided_slice_1'\nslice index -12 of dimension 0 out of bounds.\n\t [[{{node strided_slice_1}}]] [Op:__inference_test_function_27418]"
     ]
    }
   ],
   "source": [
    "gan.evaluate(valid_X, valid_y, batch_size=8192)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
