{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f79dab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 15:50:20.079502: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-13 15:50:21.317642: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "BATCH_SIZE = 8192\n",
    "LATENT_DIM = 32\n",
    "VALID_FRAC = 0.25\n",
    "SEED = 101588\n",
    "NUM_TRAIN_SAMPLES = 5000000\n",
    "\n",
    "GEN_UPDATES = 8\n",
    "D_LR = 0.0003\n",
    "G_LR = 0.0008\n",
    "FPR_THRESH = 1e-5\n",
    "\n",
    "BACKGROUND_FNAME = Path(\"background.h5\")\n",
    "SIGNAL_FNAMES = {\n",
    "    \"A-4_leptons\": \"https://zenodo.org/record/7152590/files/Ato4l_lepFilter_13TeV_filtered.h5?download=1\",\n",
    "    \"leptoquarks-b_tau\": \"https://zenodo.org/record/7152599/files/leptoquark_LOWMASS_lepFilter_13TeV_filtered.h5?download=1\",\n",
    "    \"h_0-tau_tau\": \"https://zenodo.org/record/7152614/files/hToTauTau_13TeV_PU20_filtered.h5?download=1\",\n",
    "    \"h_plus-tau_nu\": \"https://zenodo.org/record/7152617/files/hChToTauNu_13TeV_PU20_filtered.h5?download=1\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14753bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_dataset(fname, url):\n",
    "    subprocess.run(f\"wget -O {fname} {url}\", shell=True)\n",
    "    \n",
    "if not BACKGROUND_FNAME.exists():\n",
    "    download_dataset(\n",
    "        str(BACKGROUND_FNAME),\n",
    "        \"https://zenodo.org/record/5046428/files/background_for_training.h5?download=1\"\n",
    "    )\n",
    "\n",
    "for signal, url in SIGNAL_FNAMES.items():\n",
    "    fname = signal + \".h5\"\n",
    "    if not Path(fname).exists():\n",
    "        download_dataset(fname, url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58a3025a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: Pt, Eta, Phi, Class\n",
      "Event types: MET_class_1, Four_Ele_class_2, Four_Mu_class_3, Ten_Jet_class_4\n",
      "Total background events: 13451915\n",
      "Loading 5000000 events\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(dataset, N):\n",
    "    X = dataset[:N]\n",
    "    X, y = np.split(X, [3], axis=-1)\n",
    "    return X, y[:, :, 0]\n",
    "\n",
    "\n",
    "with h5py.File(BACKGROUND_FNAME, \"r\") as f:\n",
    "    print(\"Features:\", \", \".join([i.decode() for i in f[\"Particles_Names\"][:]]))\n",
    "    print(\"Event types:\", \", \".join([i.decode() for i in f[\"Particles_Classes\"][:]]))\n",
    "    print(\"Total background events:\", len(f[\"Particles\"]))\n",
    "    print(f\"Loading {NUM_TRAIN_SAMPLES} events\")\n",
    "    X, masks = load_dataset(f[\"Particles\"], NUM_TRAIN_SAMPLES)\n",
    "\n",
    "_, num_events, num_features = X.shape\n",
    "FEATURE_DIM = num_events * num_features\n",
    "X = X.reshape(-1, FEATURE_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f97b9fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    train_bg_events,\n",
    "    valid_bg_events,\n",
    "    train_bg_masks,\n",
    "    valid_bg_masks\n",
    ") = train_test_split(X, masks, test_size=VALID_FRAC, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a6fa374",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(train_bg_events)\n",
    "\n",
    "def preprocess(X, mask):\n",
    "    X = scaler.transform(X)\n",
    "    X = X.reshape(-1, num_events, num_features)\n",
    "    X[mask == 0] *= 0\n",
    "    X = X.reshape(-1, FEATURE_DIM)\n",
    "    return X\n",
    "\n",
    "train_bg_events = preprocess(train_bg_events, train_bg_masks)\n",
    "valid_bg_events = preprocess(valid_bg_events, valid_bg_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e401d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 13992 events from signal A-4_leptons\n",
      "Loading 85136 events from signal leptoquarks-b_tau\n",
      "Loading 172820 events from signal h_0-tau_tau\n",
      "Loading 190068 events from signal h_plus-tau_nu\n"
     ]
    }
   ],
   "source": [
    "valid_signal_events = []\n",
    "valid_signal_masks = []\n",
    "\n",
    "for signal in SIGNAL_FNAMES:\n",
    "    with h5py.File(signal + \".h5\", \"r\") as f:\n",
    "        dataset = f[\"Particles\"]\n",
    "        n = int(0.25 * len(dataset))\n",
    "        print(f\"Loading {n} events from signal {signal}\")\n",
    "        events, masks = load_dataset(dataset, n)\n",
    "    events = events.reshape(-1, FEATURE_DIM)\n",
    "    events = preprocess(events, masks)\n",
    "    valid_signal_events.append(events)\n",
    "    valid_signal_masks.append(masks)\n",
    "\n",
    "valid_signal_events = np.concatenate(valid_signal_events)\n",
    "valid_signal_masks = np.concatenate(valid_signal_masks)\n",
    "\n",
    "valid_y = np.concatenate([\n",
    "    np.zeros((len(valid_bg_events), 1)),\n",
    "    np.ones((len(valid_signal_events), 1))\n",
    "])\n",
    "valid_events = np.concatenate([valid_bg_events, valid_signal_events])\n",
    "valid_masks = np.concatenate([valid_bg_masks, valid_signal_masks])\n",
    "\n",
    "idx = np.random.permutation(len(valid_y))\n",
    "valid_y = valid_y[idx]\n",
    "valid_events = valid_events[idx]\n",
    "valid_masks = valid_masks[idx]\n",
    "valid_X = (valid_events, valid_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ae6da00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-13 15:50:55.559566: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "generator_input = tf.keras.Input((LATENT_DIM,))\n",
    "x = tf.keras.layers.Dense(128, activation=\"relu\")(generator_input)\n",
    "x = tf.keras.layers.Dense(256, activation=\"relu\")(x)\n",
    "generator_output = tf.keras.layers.Dense(FEATURE_DIM, activation=\"linear\")(x)\n",
    "generator_mask = tf.keras.layers.Dense(num_events, activation=\"sigmoid\")(x)\n",
    "generator = tf.keras.Model(\n",
    "    inputs=generator_input,\n",
    "    outputs=[generator_output, generator_mask]\n",
    ")\n",
    "\n",
    "discriminator_input = tf.keras.Input((FEATURE_DIM,))\n",
    "disc_x = tf.keras.layers.Dense(256, activation=\"relu\")(discriminator_input)\n",
    "disc_x = tf.keras.layers.Dense(128, activation=\"relu\")(disc_x)\n",
    "disc_x = tf.keras.layers.Dense(64, activation=\"relu\")(disc_x)\n",
    "\n",
    "discriminator_mask = tf.keras.Input((num_events,))\n",
    "disc_mask = tf.keras.layers.Dense(256, activation=\"relu\")(discriminator_mask)\n",
    "disc_mask = tf.keras.layers.Dense(128, activation=\"relu\")(disc_mask)\n",
    "disc_mask = tf.keras.layers.Dense(64, activation=\"relu\")(disc_mask)\n",
    "\n",
    "disc_x = tf.keras.layers.Concatenate()([disc_x, disc_mask])\n",
    "disc_x = tf.keras.layers.Dense(256, activation=\"relu\")(disc_x)\n",
    "disc_x = tf.keras.layers.Dense(512, activation=\"relu\")(disc_x)\n",
    "disc_x = tf.keras.layers.Dense(1, activation=\"linear\")(disc_x)\n",
    "discriminator = tf.keras.Model(\n",
    "    inputs=[discriminator_input, discriminator_mask],\n",
    "    outputs=disc_x\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a231c40",
   "metadata": {},
   "source": [
    "Can't seem to get this custom metric to work, so using a custom training loop below. But someone smarter than I should figure this out so we can just use `Model.fit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c4bdf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TPR(tf.keras.metrics.Metric):\n",
    "    def __init__(self, k, **kwargs):\n",
    "        self.k = k\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def reset_state(self):\n",
    "        # TODO: maybe these states need to be registered as variables?\n",
    "        # Also not sure what happens with `reset_state` if we don't pass\n",
    "        # the metric to `Model.compile`\n",
    "        self.background_preds = tf.convert_to_tensor(())\n",
    "        self.signal_preds = tf.convert_to_tensor(())\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        background = y_pred[y_true == 0]\n",
    "        self.background_preds = tf.concat([self.background_preds, background], axis=0)\n",
    "\n",
    "        signal = y_pred[y_true == 1]\n",
    "        self.signal_preds = tf.concat([self.signal_preds, signal], axis=0)\n",
    "\n",
    "    def result(self):\n",
    "        k = tf.shape(self.background_preds)[0] - self.k\n",
    "        threshold = tf.sort(self.background_preds)[k]\n",
    "        mask = self.signal_preds > threshold\n",
    "        mask = tf.cast(mask, tf.int64)\n",
    "        tpr = tf.math.reduce_mean(mask)\n",
    "        return tpr\n",
    "\n",
    "threshold_k = int(FPR_THRESH * len(valid_bg_events))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bb36f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Callback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_start(self, epoch, logs):\n",
    "        self.model.tpr.reset_state()\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def fudge_mask(mask, noisy=True):\n",
    "    mask = tf.where(mask == 0, -10., 10.)\n",
    "    if noisy:\n",
    "        mask = mask + tf.random.normal(shape=mask.shape)\n",
    "    return tf.sigmoid(mask)\n",
    "\n",
    "\n",
    "class GAN(tf.keras.Model):\n",
    "    def __init__(self, discriminator, generator, latent_dim, gen_updates):\n",
    "        super().__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "        self.gen_updates = gen_updates\n",
    "\n",
    "        self.d_loss_tracker = tf.keras.metrics.Mean(name=\"d_loss\")\n",
    "        self.g_loss_tracker = tf.keras.metrics.Mean(name=\"g_loss\")\n",
    "        self.tpr = tf.keras.metrics.SensitivityAtSpecificity(1 - FPR_THRESH)\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
    "        super().compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "    def step_generator(self, batch_size):\n",
    "        # Sample random points in the latent space\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "\n",
    "        # Assemble labels that say \"all real images\"\n",
    "        misleading_labels = tf.zeros((batch_size, 1))\n",
    "\n",
    "        # Train the generator (note that we should *not* update the weights\n",
    "        # of the discriminator)!\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = discriminator(generator(random_latent_vectors))\n",
    "            g_loss = loss_fn(misleading_labels, predictions)\n",
    "        grads = tape.gradient(g_loss, generator.trainable_weights)\n",
    "        g_optimizer.apply_gradients(zip(grads, generator.trainable_weights))\n",
    "        return g_loss\n",
    "\n",
    "    def train_step(self, X):\n",
    "        real_events, real_mask = X\n",
    "        batch_size = tf.shape(real_events)[0]\n",
    "\n",
    "        # train the generator for multiple steps\n",
    "        # in between a single step of the discriminator\n",
    "        g_loss = 0\n",
    "        for i in range(self.gen_updates):\n",
    "            g_loss += self.step_generator(batch_size)\n",
    "        g_loss /= self.gen_updates\n",
    "\n",
    "        # Sample random points in the latent space\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "\n",
    "        # Decode them to fake events\n",
    "        generated_events, generated_mask = generator(random_latent_vectors)\n",
    "\n",
    "        # Combine them with real events\n",
    "        combined_events = tf.concat([generated_events, real_events], axis=0)\n",
    "        combined_masks = tf.concat([generated_mask, real_mask], axis=0)\n",
    "\n",
    "        # Assemble labels discriminating real from fake events\n",
    "        labels = tf.concat(\n",
    "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
    "        )\n",
    "\n",
    "        # Add random noise to the labels - important trick!\n",
    "        labels += 0.05 * tf.random.uniform((2 * batch_size, 1))\n",
    "\n",
    "        # Train the discriminator\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = discriminator([combined_events, combined_masks])\n",
    "            d_loss = loss_fn(labels, predictions)\n",
    "        grads = tape.gradient(d_loss, discriminator.trainable_weights)\n",
    "        d_optimizer.apply_gradients(zip(grads, discriminator.trainable_weights))\n",
    "\n",
    "        self.d_loss_tracker.update_state(d_loss)\n",
    "        self.g_loss_tracker.update_state(g_loss)\n",
    "        return {\n",
    "            \"d_loss\": self.d_loss_tracker.result(),\n",
    "            \"g_loss\": self.g_loss_tracker.result(),\n",
    "        }\n",
    "\n",
    "    def test_step(self, data):\n",
    "        # Unpack the data\n",
    "        x, y = data\n",
    "        events, masks = x\n",
    "        masks = fudge_mask(masks, noisy=False)\n",
    "\n",
    "        # Compute predictions\n",
    "        y_pred = self.discriminator((events, masks), training=False)\n",
    "\n",
    "        # Updates the metrics tracking the loss\n",
    "        self.tpr.update_state(y, y_pred)\n",
    "        return {\"tpr\": self.tpr.result()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1b030f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_optimizer = tf.keras.optimizers.Adam(learning_rate=D_LR)\n",
    "g_optimizer = tf.keras.optimizers.Adam(learning_rate=G_LR)\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "\n",
    "gan = GAN(discriminator, generator, LATENT_DIM, GEN_UPDATES)\n",
    "gan.compile(d_optimizer, g_optimizer, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7d4df33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from contextlib import contextmanager\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def progbar(dataset, epoch):\n",
    "    with tqdm(dataset, desc=f\"Epoch {epoch + 1}\") as pbar:\n",
    "        yield pbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faaa13bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|█████████████████████████████████████| 458/458 [02:39<00:00,  2.88it/s, discriminator_loss=0.722, generator_loss=0.712]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator Loss: 7.218e-01, Generator Loss 7.116e-01:\n",
      "Valid TPR: 1.273e-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:   3%|▉                                     | 12/458 [00:04<02:26,  3.05it/s, discriminator_loss=0.717, generator_loss=0.699]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "events = tf.data.Dataset.from_tensor_slices(train_bg_events.astype(\"float32\"))\n",
    "masks = tf.data.Dataset.from_tensor_slices(train_bg_masks.astype(\"float32\"))\n",
    "masks = masks.map(fudge_mask)\n",
    "\n",
    "dataset = tf.data.Dataset.zip((events, masks))\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(BATCH_SIZE)\n",
    "\n",
    "losses, tprs = [], []\n",
    "for epoch in range(100):\n",
    "    epoch_losses = [0, 0]\n",
    "    with tqdm(dataset, desc=f\"Epoch {epoch + 1}\") as pbar:\n",
    "        for step, x in enumerate(pbar):\n",
    "            losses = gan.train_on_batch(*x)\n",
    "            epoch_losses = [i + j for i, j in zip(losses, epoch_losses)]\n",
    "            pbar.set_postfix(\n",
    "                discriminator_loss=epoch_losses[0] / (step + 1) ,\n",
    "                generator_loss=epoch_losses[1] / (step + 1)\n",
    "            )\n",
    "        \n",
    "    epoch_losses = [i / len(dataset) for i in epoch_losses]\n",
    "    print(\n",
    "        \"Discriminator Loss: {:0.3e}, Generator Loss {:0.3e}\".format(\n",
    "            *epoch_losses\n",
    "        )\n",
    "    )\n",
    "    losses.append(epoch_losses)\n",
    "\n",
    "    masks = fudge_mask(valid_masks, noisy=False)\n",
    "    y_pred = discriminator((valid_events, masks), training=False).numpy()\n",
    "    thresh = np.sort(y_pred[valid_y == 0])[-threshold_k]\n",
    "    tpr = (y_pred[valid_y == 1] >= thresh).mean()\n",
    "    print(f\"Valid TPR: {tpr:0.3e}\")\n",
    "    tprs.append(tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7845f3ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
